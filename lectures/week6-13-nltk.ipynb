{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "- natural language toolkit \n",
    "- very popular, easy to use\n",
    "- excellent example of integrating functionality with Python\n",
    "- there is an excellent tutorial [book](http://www.nltk.org/book/) about NLTK\n",
    "- you can learn quite a bit about NLP reading the book and playing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gui that downloads data - comes up in separate window\n",
    "# have to search for the window!\n",
    "# can just grab files for book, \n",
    "# or everything if you want - several gigs\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense = text2\n",
    "inaug = text4\n",
    "chat = text5\n",
    "monty = text6\n",
    "wsj = text7\n",
    "personals = text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows matches with some context\n",
    "\n",
    "inaug.concordance('greatness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense.concordance('affection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense.count('affection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "inaug.dispersion_plot(\\\n",
    "        [\"citizens\", \"democracy\", \n",
    "         \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total word count\n",
    "# NLTK defined __len__ to be # of words\n",
    "\n",
    "[len(wsj), type(wsj)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique word count - no dups in a set\n",
    "\n",
    "len(set(wsj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 20 random(because sets are unordered) words\n",
    "\n",
    "list(set(wsj))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort, then take a subset\n",
    "\n",
    "list(sorted(set(wsj)))[5000:5010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can ask how often certain words appear\n",
    "\n",
    "[wsj.count(t) for t in ['business', 'debt', 'inflation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a word frequency distribution\n",
    "\n",
    "fd=FreqDist(wsj)\n",
    "fd.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can query the frequency distribution like a dictionary\n",
    "\n",
    "[fd['as'], fd['apartment'], fd['apart'],\n",
    " fd['any'], wsj.count('any')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on average, each word used 8 times\n",
    "\n",
    "len(wsj)/len(set(wsj)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find long words used as many times as the number of \n",
    "# characters in the word. print a count and the words\n",
    "# that work at each length\n",
    "\n",
    "for wlen in range(8, 15):\n",
    "    # set comprehension\n",
    "    words = [w for w in set(wsj) if len(w) == wlen and wlen == fd[w]]\n",
    "    print(wlen, len(words), words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collocations are word pairs that occur more likely than \n",
    "# indivigual word frequency would suggest\n",
    "# singles text...\n",
    "\n",
    "print(len(personals))\n",
    "#personals.collocations()\n",
    "print('; '.join(personals.collocation_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wsj))\n",
    "#wsj.collocations()\n",
    "print('; '.join(wsj.collocation_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = nltk.word_tokenize('''Jack always gets very nervous and stressed \n",
    "during the last three weeks of the Columbia term, \n",
    "but Jill actually enjoys the tension and deadlines!''')\n",
    "jj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nltk.pos_tag(jj)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can never remember what the tags means\n",
    "# extract the tags, make a set to get rid of dups, sort, and\n",
    "# get tag help\n",
    "\n",
    "for tag in sorted(set([tag for word, tag in out])):\n",
    "    nltk.help.upenn_tagset(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find anagrams and palindromes in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sorted version of word to use as a key\n",
    "\n",
    "''.join(sorted('sdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the brown corpus:\n",
    "# The Brown Corpus was the first million-word electronic corpus of English, \n",
    "# created in 1961 at Brown University. This corpus contains text from 500 sources, \n",
    "# and the sources have been categorized by genre, such as news, editorial, and so on\n",
    "\n",
    "anas = collections.defaultdict(list)\n",
    "brownwords = nltk.corpus.brown.words()\n",
    "brownset = set(brownwords)\n",
    "pals = []\n",
    "\n",
    "\n",
    "for w in brownset:\n",
    "    # make anagram dict\n",
    "    \n",
    "    # make a key\n",
    "    anas[''.join(sorted(w))].append(w)\n",
    "    # check for palindromes\n",
    "    if w == w[::-1]:\n",
    "        pals.append(w)\n",
    "        \n",
    "# get rid of all digit words,\n",
    "# and words with no anagrams\n",
    "bad = [k for k in anas if len(anas[k]) == 1 or k.isdigit()]\n",
    "for b in bad:\n",
    "    del anas[b]\n",
    "        \n",
    "# make a list for random.choice\n",
    "keys = [k for k in anas]\n",
    "\n",
    "def find_anagram():\n",
    "    k = random.choice(keys)\n",
    "    return anas[k]\n",
    "\n",
    "[len(brownwords), len(brownset), len(pals), len(keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p for p in pals if len(p) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(15):\n",
    "    print(find_anagram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than 3 anagrams!!\n",
    "\n",
    "more=[anas[k]  for k in anas if len(anas[k])>3]\n",
    "sorted(more, key=lambda x : len(x), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
