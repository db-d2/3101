{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn\n",
    "- great machine learning package\n",
    "- [examples](https://scikit-learn.org/stable/auto_examples/index.html)\n",
    "- [doc](https://scikit-learn.org/stable/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Visualizing the stock market structure](https://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html#sphx-glr-download-auto-examples-applications-plot-stock-market-py)\n",
    "\n",
    "This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes.\n",
    "\n",
    "The quantity that we use is the daily variation in quote price: quotes that are linked tend to cofluctuate during a day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import cluster, covariance, manifold\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Retrieve the data from Internet\n",
    "\n",
    "# The data is from 2003 - 2008. This is reasonably calm: (not too long ago so\n",
    "# that we get high-tech firms, and before the 2008 crash). This kind of\n",
    "# historical data can be obtained for from APIs like the quandl.com and\n",
    "# alphavantage.co ones.\n",
    "\n",
    "symbol_dict = {\n",
    "    'TOT': 'Total',\n",
    "    'XOM': 'Exxon',\n",
    "    'CVX': 'Chevron',\n",
    "    'COP': 'ConocoPhillips',\n",
    "    'VLO': 'Valero Energy',\n",
    "    'MSFT': 'Microsoft',\n",
    "    'IBM': 'IBM',\n",
    "    'TWX': 'Time Warner',\n",
    "    'CMCSA': 'Comcast',\n",
    "    'CVC': 'Cablevision',\n",
    "    'YHOO': 'Yahoo',\n",
    "    'DELL': 'Dell',\n",
    "    'HPQ': 'HP',\n",
    "    'AMZN': 'Amazon',\n",
    "    'TM': 'Toyota',\n",
    "    'CAJ': 'Canon',\n",
    "    'SNE': 'Sony',\n",
    "    'F': 'Ford',\n",
    "    'HMC': 'Honda',\n",
    "    'NAV': 'Navistar',\n",
    "    'NOC': 'Northrop Grumman',\n",
    "    'BA': 'Boeing',\n",
    "    'KO': 'Coca Cola',\n",
    "    'MMM': '3M',\n",
    "    'MCD': 'McDonald\\'s',\n",
    "    'PEP': 'Pepsi',\n",
    "    'K': 'Kellogg',\n",
    "    'UN': 'Unilever',\n",
    "    'MAR': 'Marriott',\n",
    "    'PG': 'Procter Gamble',\n",
    "    'CL': 'Colgate-Palmolive',\n",
    "    'GE': 'General Electrics',\n",
    "    'WFC': 'Wells Fargo',\n",
    "    'JPM': 'JPMorgan Chase',\n",
    "    'AIG': 'AIG',\n",
    "    'AXP': 'American express',\n",
    "    'BAC': 'Bank of America',\n",
    "    'GS': 'Goldman Sachs',\n",
    "    'AAPL': 'Apple',\n",
    "    'SAP': 'SAP',\n",
    "    'CSCO': 'Cisco',\n",
    "    'TXN': 'Texas Instruments',\n",
    "    'XRX': 'Xerox',\n",
    "    'WMT': 'Wal-Mart',\n",
    "    'HD': 'Home Depot',\n",
    "    'GSK': 'GlaxoSmithKline',\n",
    "    'PFE': 'Pfizer',\n",
    "    'SNY': 'Sanofi-Aventis',\n",
    "    'NVS': 'Novartis',\n",
    "    'KMB': 'Kimberly-Clark',\n",
    "    'R': 'Ryder',\n",
    "    'GD': 'General Dynamics',\n",
    "    'RTN': 'Raytheon',\n",
    "    'CVS': 'CVS',\n",
    "    'CAT': 'Caterpillar',\n",
    "    'DD': 'DuPont de Nemours'}\n",
    "\n",
    "\n",
    "symbols, names = np.array(sorted(symbol_dict.items())).T\n",
    "\n",
    "quotes = []\n",
    "\n",
    "for symbol in symbols:\n",
    "    print('Fetching quote history for %r' % symbol, file=sys.stderr)\n",
    "    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'\n",
    "           'master/financial-data/{}.csv')\n",
    "    quotes.append(pd.read_csv(url.format(symbol)))\n",
    "\n",
    "close_prices = np.vstack([q['close'] for q in quotes])\n",
    "open_prices = np.vstack([q['open'] for q in quotes])\n",
    "\n",
    "# The daily variations of the quotes are what carry most information\n",
    "variation = close_prices - open_prices\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Learn a graphical structure from the correlations\n",
    "edge_model = covariance.GraphicalLassoCV()\n",
    "\n",
    "# standardize the time series: using correlations rather than covariance\n",
    "# is more efficient for structure recovery\n",
    "X = variation.copy().T\n",
    "X /= X.std(axis=0)\n",
    "edge_model.fit(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Cluster using affinity propagation\n",
    "\n",
    "_, labels = cluster.affinity_propagation(edge_model.covariance_)\n",
    "n_labels = labels.max()\n",
    "\n",
    "for i in range(n_labels + 1):\n",
    "    print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))\n",
    "\n",
    "# #############################################################################\n",
    "# Find a low-dimension embedding for visualization: find the best position of\n",
    "# the nodes (the stocks) on a 2D plane\n",
    "\n",
    "# We use a dense eigen_solver to achieve reproducibility (arpack is\n",
    "# initiated with random vectors that we don't control). In addition, we\n",
    "# use a large number of neighbors to capture the large-scale structure.\n",
    "node_position_model = manifold.LocallyLinearEmbedding(\n",
    "    n_components=2, eigen_solver='dense', n_neighbors=6)\n",
    "\n",
    "embedding = node_position_model.fit_transform(X.T).T\n",
    "\n",
    "# #############################################################################\n",
    "# Visualization\n",
    "plt.figure(1, facecolor='w', figsize=(10, 8))\n",
    "plt.clf()\n",
    "ax = plt.axes([0., 0., 1., 1.])\n",
    "plt.axis('off')\n",
    "\n",
    "# Display a graph of the partial correlations\n",
    "partial_correlations = edge_model.precision_.copy()\n",
    "d = 1 / np.sqrt(np.diag(partial_correlations))\n",
    "partial_correlations *= d\n",
    "partial_correlations *= d[:, np.newaxis]\n",
    "non_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n",
    "\n",
    "# Plot the nodes using the coordinates of our embedding\n",
    "plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n",
    "            cmap=plt.cm.nipy_spectral)\n",
    "\n",
    "# Plot the edges\n",
    "start_idx, end_idx = np.where(non_zero)\n",
    "# a sequence of (*line0*, *line1*, *line2*), where::\n",
    "#            linen = (x0, y0), (x1, y1), ... (xm, ym)\n",
    "segments = [[embedding[:, start], embedding[:, stop]]\n",
    "            for start, stop in zip(start_idx, end_idx)]\n",
    "values = np.abs(partial_correlations[non_zero])\n",
    "lc = LineCollection(segments,\n",
    "                    zorder=0, cmap=plt.cm.hot_r,\n",
    "                    norm=plt.Normalize(0, .7 * values.max()))\n",
    "lc.set_array(values)\n",
    "lc.set_linewidths(15 * values)\n",
    "ax.add_collection(lc)\n",
    "\n",
    "# Add a label to each node. The challenge here is that we want to\n",
    "# position the labels to avoid overlap with other labels\n",
    "for index, (name, label, (x, y)) in enumerate(\n",
    "        zip(names, labels, embedding.T)):\n",
    "\n",
    "    dx = x - embedding[0]\n",
    "    dx[index] = 1\n",
    "    dy = y - embedding[1]\n",
    "    dy[index] = 1\n",
    "    this_dx = dx[np.argmin(np.abs(dy))]\n",
    "    this_dy = dy[np.argmin(np.abs(dx))]\n",
    "    if this_dx > 0:\n",
    "        horizontalalignment = 'left'\n",
    "        x = x + .002\n",
    "    else:\n",
    "        horizontalalignment = 'right'\n",
    "        x = x - .002\n",
    "    if this_dy > 0:\n",
    "        verticalalignment = 'bottom'\n",
    "        y = y + .002\n",
    "    else:\n",
    "        verticalalignment = 'top'\n",
    "        y = y - .002\n",
    "    plt.text(x, y, name, size=10,\n",
    "             horizontalalignment=horizontalalignment,\n",
    "             verticalalignment=verticalalignment,\n",
    "             bbox=dict(facecolor='w',\n",
    "                       edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),\n",
    "                       alpha=.6))\n",
    "\n",
    "plt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n",
    "         embedding[0].max() + .10 * embedding[0].ptp(),)\n",
    "plt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n",
    "         embedding[1].max() + .03 * embedding[1].ptp())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# [Face completion with a multi-output estimators](https://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py)\n",
    "\n",
    "\n",
    "This example shows the use of multi-output estimator to complete images.\n",
    "The goal is to predict the lower half of a face given its upper half.\n",
    "\n",
    "The first column of images shows true faces. The next columns illustrate\n",
    "how extremely randomized trees, k nearest neighbors, linear\n",
    "regression and ridge regression complete the lower half of those faces.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.utils.validation import check_random_state\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Load the faces datasets\n",
    "data = fetch_olivetti_faces()\n",
    "targets = data.target\n",
    "\n",
    "data = data.images.reshape((len(data.images), -1))\n",
    "train = data[targets < 30]\n",
    "test = data[targets >= 30]  # Test on independent people\n",
    "\n",
    "# Test on a subset of people\n",
    "n_faces = 5\n",
    "rng = check_random_state(4)\n",
    "face_ids = rng.randint(test.shape[0], size=(n_faces, ))\n",
    "test = test[face_ids, :]\n",
    "\n",
    "n_pixels = data.shape[1]\n",
    "# Upper half of the faces\n",
    "X_train = train[:, :(n_pixels + 1) // 2]\n",
    "# Lower half of the faces\n",
    "y_train = train[:, n_pixels // 2:]\n",
    "X_test = test[:, :(n_pixels + 1) // 2]\n",
    "y_test = test[:, n_pixels // 2:]\n",
    "\n",
    "# Fit estimators\n",
    "ESTIMATORS = {\n",
    "    \"Extra trees\": ExtraTreesRegressor(n_estimators=10, max_features=32,\n",
    "                                       random_state=0),\n",
    "    \"K-nn\": KNeighborsRegressor(),\n",
    "    \"Linear regression\": LinearRegression(),\n",
    "    \"Ridge\": RidgeCV(),\n",
    "}\n",
    "\n",
    "y_test_predict = dict()\n",
    "for name, estimator in ESTIMATORS.items():\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_test_predict[name] = estimator.predict(X_test)\n",
    "\n",
    "# Plot the completed faces\n",
    "image_shape = (64, 64)\n",
    "\n",
    "n_cols = 1 + len(ESTIMATORS)\n",
    "plt.figure(figsize=(2. * n_cols, 2.26 * n_faces))\n",
    "plt.suptitle(\"Face completion with multi-output estimators\", size=16)\n",
    "\n",
    "for i in range(n_faces):\n",
    "    true_face = np.hstack((X_test[i], y_test[i]))\n",
    "\n",
    "    if i:\n",
    "        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)\n",
    "    else:\n",
    "        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1,\n",
    "                          title=\"true faces\")\n",
    "\n",
    "    sub.axis(\"off\")\n",
    "    sub.imshow(true_face.reshape(image_shape),\n",
    "               cmap=plt.cm.gray,\n",
    "               interpolation=\"nearest\")\n",
    "\n",
    "    for j, est in enumerate(sorted(ESTIMATORS)):\n",
    "        completed_face = np.hstack((X_test[i], y_test_predict[est][i]))\n",
    "\n",
    "        if i:\n",
    "            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)\n",
    "\n",
    "        else:\n",
    "            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j,\n",
    "                              title=est)\n",
    "\n",
    "        sub.axis(\"off\")\n",
    "        sub.imshow(completed_face.reshape(image_shape),\n",
    "                   cmap=plt.cm.gray,\n",
    "                   interpolation=\"nearest\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
